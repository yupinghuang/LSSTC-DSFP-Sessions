{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "%pylab inline\n",
    "%config InlineBackend.figure_format = 'retina'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import arviz as az\n",
    "import pandas as pd\n",
    "import pystan\n",
    "import seaborn as sns\n",
    "from io import StringIO\n",
    "\n",
    "sns.set_context('notebook')\n",
    "sns.set_palette('colorblind')\n",
    "sns.set_style('ticks')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_fitted_line(fit, x, y, xerr=None, yerr=None):\n",
    "    xmin = np.min(x)\n",
    "    xmax = np.max(x)\n",
    "    \n",
    "    xmin, xmax = (xmin - 0.1*(xmax-xmin)), (xmax + 0.1*(xmax-xmin))\n",
    "    \n",
    "    xs = linspace(xmin, xmax, 100)\n",
    "    ys = []\n",
    "    for m, b in zip(fit.posterior.m.values.flatten(), fit.posterior.b.values.flatten()):\n",
    "        ys.append(m*xs + b)\n",
    "    ys = array(ys)\n",
    "    \n",
    "    l, _ = plot(xs, median(ys, axis=0))\n",
    "    fill_between(xs, percentile(ys, 84, axis=0), percentile(ys, 16, axis=0), color=l.get_color(), alpha=0.25)\n",
    "    fill_between(xs, percentile(ys, 97.5, axis=0), percentile(ys, 2.5, axis=0), color=l.get_color(), alpha=0.25)\n",
    "    \n",
    "    # Plot the data\n",
    "    errorbar(x, y, xerr=xerr, yerr=yerr, color='k', fmt='.')\n",
    "    \n",
    "def plot_inferred_ys_noscatter(fit, x):\n",
    "    ys = []\n",
    "    for m, b in zip(fit.posterior.m.values.flatten(), fit.posterior.b.values.flatten()):\n",
    "        ys.append(m*x + b)\n",
    "    ys = array(ys)\n",
    "    \n",
    "    errorbar(x, mean(ys, axis=0), yerr=std(ys, axis=0), fmt='.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is the Hogg, Bovy & Lang (2010) data set (normally so good at this, they neglected to publish the data separately, so I had to scrape the PDF):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>x</th>\n",
       "      <th>y</th>\n",
       "      <th>sigma_x</th>\n",
       "      <th>sigma_y</th>\n",
       "      <th>rho_xy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>201</td>\n",
       "      <td>592</td>\n",
       "      <td>61</td>\n",
       "      <td>9</td>\n",
       "      <td>-0.84</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>244</td>\n",
       "      <td>401</td>\n",
       "      <td>25</td>\n",
       "      <td>4</td>\n",
       "      <td>0.31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>47</td>\n",
       "      <td>583</td>\n",
       "      <td>38</td>\n",
       "      <td>11</td>\n",
       "      <td>0.64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>287</td>\n",
       "      <td>402</td>\n",
       "      <td>15</td>\n",
       "      <td>7</td>\n",
       "      <td>-0.27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>203</td>\n",
       "      <td>495</td>\n",
       "      <td>21</td>\n",
       "      <td>5</td>\n",
       "      <td>-0.33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "      <td>58</td>\n",
       "      <td>173</td>\n",
       "      <td>15</td>\n",
       "      <td>9</td>\n",
       "      <td>0.67</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>7</td>\n",
       "      <td>210</td>\n",
       "      <td>479</td>\n",
       "      <td>27</td>\n",
       "      <td>4</td>\n",
       "      <td>-0.02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>8</td>\n",
       "      <td>202</td>\n",
       "      <td>504</td>\n",
       "      <td>14</td>\n",
       "      <td>4</td>\n",
       "      <td>-0.05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>9</td>\n",
       "      <td>198</td>\n",
       "      <td>510</td>\n",
       "      <td>30</td>\n",
       "      <td>11</td>\n",
       "      <td>-0.84</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>10</td>\n",
       "      <td>158</td>\n",
       "      <td>416</td>\n",
       "      <td>16</td>\n",
       "      <td>7</td>\n",
       "      <td>-0.69</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>11</td>\n",
       "      <td>165</td>\n",
       "      <td>393</td>\n",
       "      <td>14</td>\n",
       "      <td>5</td>\n",
       "      <td>0.30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>12</td>\n",
       "      <td>201</td>\n",
       "      <td>442</td>\n",
       "      <td>25</td>\n",
       "      <td>5</td>\n",
       "      <td>-0.46</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>13</td>\n",
       "      <td>157</td>\n",
       "      <td>317</td>\n",
       "      <td>52</td>\n",
       "      <td>5</td>\n",
       "      <td>-0.03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>14</td>\n",
       "      <td>131</td>\n",
       "      <td>311</td>\n",
       "      <td>16</td>\n",
       "      <td>6</td>\n",
       "      <td>0.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>15</td>\n",
       "      <td>166</td>\n",
       "      <td>400</td>\n",
       "      <td>34</td>\n",
       "      <td>6</td>\n",
       "      <td>0.73</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>16</td>\n",
       "      <td>160</td>\n",
       "      <td>337</td>\n",
       "      <td>31</td>\n",
       "      <td>5</td>\n",
       "      <td>-0.52</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>17</td>\n",
       "      <td>186</td>\n",
       "      <td>423</td>\n",
       "      <td>42</td>\n",
       "      <td>9</td>\n",
       "      <td>0.90</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>18</td>\n",
       "      <td>125</td>\n",
       "      <td>334</td>\n",
       "      <td>26</td>\n",
       "      <td>8</td>\n",
       "      <td>0.40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>19</td>\n",
       "      <td>218</td>\n",
       "      <td>533</td>\n",
       "      <td>16</td>\n",
       "      <td>6</td>\n",
       "      <td>-0.78</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>20</td>\n",
       "      <td>146</td>\n",
       "      <td>344</td>\n",
       "      <td>22</td>\n",
       "      <td>5</td>\n",
       "      <td>-0.56</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    ID    x    y  sigma_x  sigma_y  rho_xy\n",
       "0    1  201  592       61        9   -0.84\n",
       "1    2  244  401       25        4    0.31\n",
       "2    3   47  583       38       11    0.64\n",
       "3    4  287  402       15        7   -0.27\n",
       "4    5  203  495       21        5   -0.33\n",
       "5    6   58  173       15        9    0.67\n",
       "6    7  210  479       27        4   -0.02\n",
       "7    8  202  504       14        4   -0.05\n",
       "8    9  198  510       30       11   -0.84\n",
       "9   10  158  416       16        7   -0.69\n",
       "10  11  165  393       14        5    0.30\n",
       "11  12  201  442       25        5   -0.46\n",
       "12  13  157  317       52        5   -0.03\n",
       "13  14  131  311       16        6    0.50\n",
       "14  15  166  400       34        6    0.73\n",
       "15  16  160  337       31        5   -0.52\n",
       "16  17  186  423       42        9    0.90\n",
       "17  18  125  334       26        8    0.40\n",
       "18  19  218  533       16        6   -0.78\n",
       "19  20  146  344       22        5   -0.56"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hogg_data = pd.read_csv(StringIO(\"\"\"ID\tx\ty\tsigma_x\tsigma_y\trho_xy\n",
    "1\t201\t592\t61\t9\t-0.84\n",
    "2\t244\t401\t25\t4\t0.31\n",
    "3\t47\t583\t38\t11\t0.64\n",
    "4\t287\t402\t15\t7\t-0.27\n",
    "5\t203\t495\t21\t5\t-0.33\n",
    "6\t58\t173\t15\t9\t0.67\n",
    "7\t210\t479\t27\t4\t-0.02\n",
    "8\t202\t504\t14\t4\t-0.05\n",
    "9\t198\t510\t30\t11\t-0.84\n",
    "10\t158\t416\t16\t7\t-0.69\n",
    "11\t165\t393\t14\t5\t0.30\n",
    "12\t201\t442\t25\t5\t-0.46\n",
    "13\t157\t317\t52\t5\t-0.03\n",
    "14\t131\t311\t16\t6\t0.50\n",
    "15\t166\t400\t34\t6\t0.73\n",
    "16\t160\t337\t31\t5\t-0.52\n",
    "17\t186\t423\t42\t9\t0.90\n",
    "18\t125\t334\t26\t8\t0.40\n",
    "19\t218\t533\t16\t6\t-0.78\n",
    "20\t146\t344\t22\t5\t-0.56\n",
    "\"\"\"), sep='\\t')\n",
    "hogg_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Incorporate the uncertainty in the measurement of $x$ from [Hogg, Bovy & Lang (2010)](https://arxiv.org/pdf/1008.4686.pdf) into your model.  You will need to introduce a variable $x_\\mathrm{true}$ (just like our $y_\\mathrm{true}$) and include an observational likelihood for its values.  Make plots of your fits similar to those we have produced.  Is your model reasonable?  (Remember to fit data points 5 to 20, as the first four points are outliers!)\n",
    "\n",
    "Can you write a version of this model that uses a mixture model to fit out the outliers, as we did above?\n",
    "\n",
    "Can you extend your model to account for either intrinsic scatter or inaccuracy in the measurement uncertainties reported in both dimensions, as we did before?  Could you account for both effects in this data set?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cosmology!  Download the Type Ia supernova dataset from [Scolnic, et al. (2017)](https://arxiv.org/abs/1710.00845); you can find it [here](https://archive.stsci.edu/hlsps/ps1cosmo/scolnic/hlsp_ps1cosmo_panstarrs_gpc1_all_model_v1_lcparam-full.txt).  It gives measurements of the redshift (assumed perfectly measured) and distance modulus (imperfectly measured) for some hundreds of supernova from the PanSTARRS survey.  Recall that the distance modulus is \n",
    "$$\n",
    "\\mu = 5 \\log_{10} \\left( \\frac{d_L}{10 \\, \\mathrm{pc}} \\right)\n",
    "$$\n",
    "and the luminosity distance (see [Hogg (1999)](https://arxiv.org/abs/astro-ph/9905116)) is given in terms of the cosmological parameters of a flat universe by \n",
    "$$\n",
    "d_L\\left( z \\mid H_0, \\Omega_M, w\\right) = \\frac{c}{H_0} \\int_{0}^z \\mathrm{d} z \\, \\frac{1}{\\sqrt{\\Omega_M \\left( 1 + z \\right)^3 + \\left( 1 - \\Omega_M \\right) \\left( 1 + z \\right)^{3(1+w)}}}.\n",
    "$$\n",
    "\n",
    "This is a *non-linear* version of the line fitting problem: we have a perfectly-measured redshift; we use cosmology---rather than a linear relation---to predict the luminosity distance, which predicts the distance modulus.  We then compare to the observed distance modulus (plus uncertainty!) to constrain the cosmology.  You can either learn about how to do integrals like the above in Stan (see [here](https://mc-stan.org/docs/2_20/stan-users-guide/integrate-1d.html); but you will need to install a newer version of Stan and do some fancy coding---this is expert-mode!) or you can use the following rational function approximation to $d_L$ that is good to $z \\simeq 1.5$ or so:\n",
    "$$\n",
    "d_L = \\frac{c}{H_0} \\frac{z + z^2\\frac{3 - 10 w +3 w^2 + 10 w \\Omega_M + 6 w^2 \\Omega_M - 9 w^2 \\Omega_M^2}{4 \\left( 1 - 3 w + 3 w \\Omega_M \\right)}}{1 + z \\frac{1 - 2 w -3 w^2 +2 w \\Omega_M + 12 w^2 \\Omega_M - 9 w^2 \\Omega_M^2}{2 \\left( 1 - 3 w + 3 w \\Omega_M\\right)}}\n",
    "$$\n",
    "\n",
    "Recall that there are some limits on variables: $0 \\leq \\Omega_M < 1$, and $0 < H_0$.  So you will need to declare these in the Stan parameters block:\n",
    "\n",
    "    real<lower=0,upper=1> Om;\n",
    "    real<lower=0> H0;\n",
    "   \n",
    "It is up to you whether you want to encode the weak energy condition $w > -1$ or not."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The paper [Pearson, et al. (2015)](https://ui.adsabs.harvard.edu/abs/2015MNRAS.449.3082P/abstract) tried to calibrate a large number of \"mass proxies\" for galaxy groups based on simulations.  One of the data sets can be found [here](https://github.com/farr/galmassproxy/blob/master/data/obs_cmproxy.csv).  \n",
    "\n",
    "You can, of course, take each proxy one-by-one in a linear fit similar to those we have been doing.  But, a better approach is to imagine that the true mass and proxies are drawn from a multivariate normal distribution.  Stan has facilities for modelling the mean vector and covariance matrix of this multivariate normal (see [here](https://mc-stan.org/docs/2_20/functions-reference/distributions-over-unbounded-vectors.html) and [here](https://mc-stan.org/docs/2_20/functions-reference/covariance-matrix-distributions.html) or [here](https://mc-stan.org/docs/2_20/functions-reference/correlation-matrix-distributions.html)).  Write down a model that has (log) true mass and some number of proxies drawn from such a MVN, and then constraints the parameters of this MVN using some number of observations with uncertainty.  Fit it to the data in the datasets directory.\n",
    "\n",
    "Now extend your model to incorporate some number of observations of only the proxy values (i.e. do not include a term in the likelihood for log mass in this subset of observations); the model can still *predict* masses from the MVN using the proxy observations and the observed common distribution.\n",
    "\n",
    "Comment on the \"meaning\" of your model; in particular, what are you assuming about the proxies in the \"training\" set compared to the \"observation\" set?  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
